### 2. Balancing the Data


```pyspark
data = spark.read.parquet('s3://amazon-reviews-pds/parquet/product_category=Books/*.parquet')
```


    VBox()


    Starting Spark application



<table>
<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1606857804574_0002</td><td>pyspark</td><td>idle</td><td><a target="_blank" href="http://ip-172-31-25-235.ec2.internal:20888/proxy/application_1606857804574_0002/">Link</a></td><td><a target="_blank" href="http://ip-172-31-27-104.ec2.internal:8042/node/containerlogs/container_1606857804574_0002_01_000001/livy">Link</a></td><td>✔</td></tr></table>



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


    SparkSession available as 'spark'.



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
data = data.filter(data.review_headline.isNotNull())
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
# Good == 1, Bad == 0 (cast as integers so that pyspark.ml can understand them)
data = data.withColumn('good_review', (data.star_rating >= 4).cast("integer"))

# Check to make sure new column is capturing star_rating correctly
data[['star_rating', 'good_review']].show(5)

# Take a look at how many good and bad reviews we have, respectively
(data.groupBy('good_review')
     .count()
     .show()
)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


    +-----------+-----------+
    |star_rating|good_review|
    +-----------+-----------+
    |          5|          1|
    |          4|          1|
    |          4|          1|
    |          5|          1|
    |          5|          1|
    +-----------+-----------+
    only showing top 5 rows
    
    +-----------+--------+
    |good_review|   count|
    +-----------+--------+
    |          1|17208399|
    |          0| 3517695|
    +-----------+--------+


```pyspark
# Downsampling the good reviews

from pyspark.sql.functions import col

sampled = data.sampleBy("good_review", fractions={0: 1, 1: 0.2044}, seed=0)
sampled.groupBy("good_review").count().orderBy("good_review").show()
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


    +-----------+-------+
    |good_review|  count|
    +-----------+-------+
    |          0|3517695|
    |          1|3517335|
    +-----------+-------+


```pyspark

```

### 3. Implementing a Reproducible Machine Learning Pipeline

#### (a)


```pyspark
print('Total Columns: %d' % len(data.dtypes))
print('Total Rows: %d' % data.count())
sampled.printSchema()
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


    Total Columns: 16
    Total Rows: 20726094
    root
     |-- marketplace: string (nullable = true)
     |-- customer_id: string (nullable = true)
     |-- review_id: string (nullable = true)
     |-- product_id: string (nullable = true)
     |-- product_parent: string (nullable = true)
     |-- product_title: string (nullable = true)
     |-- star_rating: integer (nullable = true)
     |-- helpful_votes: integer (nullable = true)
     |-- total_votes: integer (nullable = true)
     |-- vine: string (nullable = true)
     |-- verified_purchase: string (nullable = true)
     |-- review_headline: string (nullable = true)
     |-- review_body: string (nullable = true)
     |-- review_date: date (nullable = true)
     |-- year: integer (nullable = true)
     |-- good_review: integer (nullable = true)

#### Additional #1


```pyspark
from pyspark.ml.feature import StringIndexer 

vine_idx = StringIndexer(inputCol='vine', outputCol='vine_idx') 
sampled = vine_idx.fit(sampled).transform(sampled)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


Here I encodes a string column of vine to a column of indexes using StringIndexer. The column of vine refers to whether a reviewer are the most trusted. If so, then she will have a "Y" for the vine column or she will have "0" instead. The indices are ordered by the frequency of vine. By using this, I can show which reviews are more trustworthy than the others.

#### Additional #2


```pyspark
# Index verified_purchase: N -> 0, Y -> 1
verified_idx = StringIndexer(inputCol="verified_purchase", outputCol="verified_idx")
sampled = verified_idx.fit(sampled).transform(sampled)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


I also use StringIndexer to index whether the purchase is verified. That the purchase if verfied means that the customer indeed purchase that item from Amzaon and does not receive any deep discount for posting good reviews. Using these indices, I can also consider the credibility of a good review.

#### Additional #3


```pyspark
# Remove the punctuations
from pyspark.sql.functions import regexp_replace
    
# Regular expression (REGEX) to match commas and hyphens
REGEX = '[,\\-!.~$]'
sampled = sampled.withColumn('text', regexp_replace(sampled.review_headline, REGEX, ' '))
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
# Text to tokens
from pyspark.ml.feature import Tokenizer
sampled = Tokenizer(inputCol="text", outputCol="tokens").transform(sampled)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
# Remove stop words
from pyspark.ml.feature import StopWordsRemover
stopwords = StopWordsRemover()
stopwords = stopwords.setInputCol('tokens').setOutputCol('words') 
sampled = stopwords.transform(sampled)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
# Feature hasing
from pyspark.ml.feature import HashingTF
hasher = HashingTF(inputCol="words", outputCol="hash", numFeatures=32)
sampled = hasher.transform(sampled)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
sampled = sampled.filter(sampled.hash.isNotNull())
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
# Common words
from pyspark.ml.feature import IDF
sampled = IDF(inputCol="hash", outputCol="review_headline_idf").fit(sampled).transform(sampled)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


Here I compute the Term Frequency-Inverse Document Frequency (TF-IDF) for customer reviews' title. TF-IDF generally improves performance when using text as features as it captures the importance of a word to the review containing it. So using TF-IDF, I could find if there is any pattern for the titles of good reviews.


```pyspark
from pyspark.ml.feature import VectorAssembler
features = ['total_votes', 'vine_idx', 'verified_idx', "review_headline_idf"]
assembler = VectorAssembler(inputCols = features, outputCol = 'features')

sampled = assembler.transform(sampled)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
cols = ['good_review'] + features + ['features']
sampled[cols].show()
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


    +-----------+-----------+--------+------------+--------------------+--------------------+
    |good_review|total_votes|vine_idx|verified_idx| review_headline_idf|            features|
    +-----------+-----------+--------+------------+--------------------+--------------------+
    |          0|          7|     0.0|         0.0|(32,[18,22,25,30]...|(35,[0,21,25,28,3...|
    |          1|          0|     0.0|         0.0|(32,[6,24,29,30],...|(35,[9,27,32,33],...|
    |          0|          6|     0.0|         1.0|(32,[3,5,7,9,13,1...|(35,[0,2,6,8,10,1...|
    |          1|          0|     0.0|         1.0|(32,[0,8,10,13,25...|(35,[2,3,11,13,16...|
    |          1|          1|     0.0|         1.0|(32,[3,7,13,17,19...|(35,[0,2,6,10,16,...|
    |          0|          4|     0.0|         0.0|(32,[3,16,19,31],...|(35,[0,6,19,22,34...|
    |          0|          4|     0.0|         0.0|(32,[13,31],[2.22...|(35,[0,16,34],[4....|
    |          1|          0|     0.0|         1.0|(32,[6,31],[2.239...|(35,[2,9,34],[1.0...|
    |          1|          6|     0.0|         0.0|(32,[2,3,4,5,11,3...|(35,[0,5,6,7,8,14...|
    |          1|          3|     0.0|         0.0|(32,[17,24],[2.23...|(35,[0,20,27],[3....|
    |          1|          0|     0.0|         0.0|(32,[3,4,14,25],[...|(35,[6,7,17,28],[...|
    |          0|          1|     0.0|         1.0|(32,[4,26,27,28],...|(35,[0,2,7,29,30,...|
    |          0|         37|     1.0|         0.0|(32,[4,18],[2.660...|(35,[0,1,7,21],[3...|
    |          1|          4|     0.0|         0.0|(32,[6,9,17,19,28...|(35,[0,9,12,20,22...|
    |          0|          6|     0.0|         0.0|(32,[26],[2.64855...|(35,[0,29],[6.0,2...|
    |          0|          1|     0.0|         1.0|(32,[13],[2.22435...|(35,[0,2,16],[1.0...|
    |          1|          0|     0.0|         1.0|(32,[6,31],[2.239...|(35,[2,9,34],[1.0...|
    |          1|          3|     0.0|         0.0|(32,[20,27],[1.96...|(35,[0,23,30],[3....|
    |          1|          0|     0.0|         0.0|(32,[1,31],[2.914...|(35,[4,34],[2.914...|
    |          1|          2|     0.0|         0.0|(32,[20,25],[1.96...|(35,[0,23,28],[2....|
    +-----------+-----------+--------+------------+--------------------+--------------------+
    only showing top 20 rows


#### (b)


```pyspark
sampled = sampled[['total_votes', 'vine', 'verified_purchase', 'review_headline', 'good_review']]
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
# Regular expression (REGEX) to match commas and hyphens
REGEX = '[,\\-!.~$]'
sampled = sampled.withColumn('text', regexp_replace(sampled.review_headline, REGEX, ' '))
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
from pyspark.ml.feature import VectorAssembler

vine_idx = StringIndexer(inputCol='vine', outputCol='vine_idx') 
verified_idx = StringIndexer(inputCol="verified_purchase", outputCol="verified_idx")

token = Tokenizer(inputCol="text", outputCol="tokens")
stopwords = stopwords.setInputCol('tokens').setOutputCol('words') 
hashTF = HashingTF(inputCol="words", outputCol="hash", numFeatures=32)
idf = IDF(inputCol="hash", outputCol="review_headline_idf")

features = ['total_votes', 'vine_idx', 'verified_idx', 'review_headline_idf']
assembler = VectorAssembler(inputCols = features, outputCol = 'features')
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
# Split sets
train, test = sampled.randomSplit([0.7, 0.3])
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(labelCol='good_review')
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
pipeline = Pipeline(stages=[vine_idx, verified_idx, token, stopwords, hashTF, idf, assembler, lr])
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark

```

#### (c)

When I specify this series of transformations in a pipline, these stages are run in order and the DataFrame inputted is transformed as it passes through each stage. In stages of transformaers, the transform() method is called on the input. For stages of estimators, the fit() method is called to produce a transformer, and that transformer’s transform() method is called on the input DataFrame.

My DataFrame is not actually processed when I chain together a sequence of transformations in a pipline. A pipeline is an estimator, so my DataFrame can only be processed after I call the fit() method on the pipeline like other estimators. After I call the fit() method and feed my DataFrame into the pipelline, my DataFrame will be processed in the order of stages I determined in the construction of the pipeline.

This is different to Dask's execution model because users are able to incorporate all models they want and obtain all-in-one solution, while Dask may be more comlicated to use. 


```pyspark

```

### 4. Finding Optimal Model Parameters


```pyspark
import numpy as np

from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
paramGrid = ParamGridBuilder().addGrid(lr.regParam, np.arange(0, .1, .01)).addGrid(lr.elasticNetParam, [0,1]).build()
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
evaluator = BinaryClassificationEvaluator(labelCol = 'good_review')

crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
# Run cross-validation, and choose the best set of parameters.
cvModel = crossval.setNumFolds(5).fit(train)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


    Exception in thread cell_monitor-39:
    Traceback (most recent call last):
      File "/opt/conda/lib/python3.7/threading.py", line 926, in _bootstrap_inner
        self.run()
      File "/opt/conda/lib/python3.7/threading.py", line 870, in run
        self._target(*self._args, **self._kwargs)
      File "/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py", line 178, in cell_monitor
        job_binned_stages[job_id][stage_id] = all_stages[stage_id]
    KeyError: 1076
    



```pyspark
# Make predictions on test documents. cvModel uses the best model found (lrModel).
bestModel = cvModel.bestModel
prediction = bestModel.transform(test)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
# Training Summary Data
trainingSummary = bestModel.stages[-1].summary

tested = prediction[['total_votes', 'vine_idx', 'verified_idx', 'review_headline_idf', 'good_review', 'text', 'features']]
evaluationSummary = bestModel.stages[-1].evaluate(tested)
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
print("Training AUC: " + str(trainingSummary.areaUnderROC))
print("Test AUC: ", str(evaluationSummary.areaUnderROC))

print("\nFalse positive rate by label (Training):")
for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):
    print("label %d: %s" % (i, rate))

print("\nTrue positive rate by label (Training):")
for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):
    print("label %d: %s" % (i, rate))
    
print("\nTraining Accuracy: " + str(trainingSummary.accuracy))
print("Test Accuracy: ", str(evaluationSummary.accuracy))
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


    Training AUC: 0.7043003561184973
    Test AUC:  0.703831974033098
    
    False positive rate by label (Training):
    label 0: 0.34518918883782607
    label 1: 0.35973977966982973
    
    True positive rate by label (Training):
    label 0: 0.6402602203301703
    label 1: 0.6548108111621739
    
    Training Accuracy: 0.6475350665093711
    Test Accuracy:  0.6474002918817661


```pyspark
sc.install_pypi_package("pandas==1.1.4")
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


    Collecting pandas==1.1.4
      Downloading https://files.pythonhosted.org/packages/bf/4c/cb7da76f3a5e077e545f9cf8575b8f488a4e8ad60490838f89c5cdd5bb57/pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)
    Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas==1.1.4)
    Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==1.1.4)
    Collecting python-dateutil>=2.7.3 (from pandas==1.1.4)
      Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.1.4)
    Installing collected packages: python-dateutil, pandas
    Successfully installed pandas-1.1.4 python-dateutil-2.8.1


```pyspark
# Get ROC curve and send it to Pandas so that we can plot it
roc_df = evaluationSummary.roc.toPandas()
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



```pyspark
sc.install_pypi_package("matplotlib")
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…


    Collecting matplotlib
      Downloading https://files.pythonhosted.org/packages/30/f2/10c822cb0ca5ebec58bd1892187bc3e3db64a867ac26531c6204663fc218/matplotlib-3.3.3-cp37-cp37m-manylinux1_x86_64.whl (11.6MB)
    Requirement already satisfied: numpy>=1.15 in /usr/local/lib64/python3.7/site-packages (from matplotlib)
    Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1606858747135-0/lib/python3.7/site-packages (from matplotlib)
    Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 (from matplotlib)
      Downloading https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)
    Collecting pillow>=6.2.0 (from matplotlib)
      Downloading https://files.pythonhosted.org/packages/af/fa/c1302a26d5e1a17fa8e10e43417b6cf038b0648c4b79fcf2302a4a0c5d30/Pillow-8.0.1-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)
    Collecting cycler>=0.10 (from matplotlib)
      Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl
    Collecting kiwisolver>=1.0.1 (from matplotlib)
      Downloading https://files.pythonhosted.org/packages/d2/46/231de802ade4225b76b96cffe419cf3ce52bbe92e3b092cf12db7d11c207/kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib)
    Installing collected packages: pyparsing, pillow, cycler, kiwisolver, matplotlib
    Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.3 pillow-8.0.1 pyparsing-2.4.7


```pyspark
import matplotlib.pyplot as plt

# Close previous plots; otherwise, will just overwrite and display again
plt.close()

roc_df.plot(x = 'FPR', y = 'TPR',label = 'AUC = %0.20f' % evaluationSummary.areaUnderROC)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.title('ROC Curve')

%matplot plt
```


    VBox()



    FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…



    
![png](output_47_2.png)
    


The optimal model has a test AUC of 0.703831974033098, and a test accuracy of 0.6474002918817661. The model does well in predictions as it has a relatively large true positive rate by label of 0.6548 for label "1" for the training set. More importantly, compared to Lab 6, the optimal model here increases AUC from barely above 0.5 to over 0.7.

It does poorly as it still has a relatively high false positive rate of 0.3597 for labeling the good reviews in the training set. To improve it, I would consider taking into account the impact of other attributes and use more columns for training. For example, I could count the popularity of a certain product by using StringIndexer to count the frequency of product_title in the dataset.


```pyspark

```
